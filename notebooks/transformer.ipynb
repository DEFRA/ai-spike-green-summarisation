{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d46988d",
   "metadata": {},
   "source": [
    "## Prequisites\n",
    "\n",
    "To run this notebook, you will need acesss to:\n",
    "- An Azure OpenAI resource with access to the Phi-4 model.\n",
    "- A local machine capable of running transformer models (with sufficient CPU/GPU and memory).\n",
    "\n",
    "In your working directory, create a `.env` file (if it doesn't already exist) and add the following environment variables:\n",
    "```bash\n",
    "AZURE_OPENAI_API_KEY=your_azure_openai_api_key\n",
    "AZURE_OPENAI_ENDPOINT=your_azure_openai_endpoint\n",
    "OPENAI_API_VERSION=2024-02-15\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16621c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "pathlib.Path(\".env\").touch()\n",
    "\n",
    "# Install web scraping dependencies\n",
    "%pip install beautifulsoup4 lxml markdownify\n",
    "\n",
    "%pip install seaborn matplotlib\n",
    "\n",
    "# Install AI dependencies\n",
    "%pip install \"transformers[torch]\" \"pydantic-ai-slim[openai]\" \"python-dotenv\" \"openai\" \"chonkie\"\n",
    "\n",
    "# Install Evaluation dependencies\n",
    "%pip install \"torchmetrics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4300cf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv(dotenv_path=\".env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa788b9e",
   "metadata": {},
   "source": [
    "## Setup Models\n",
    "\n",
    "In this PoC, we will be using two models for summarisation:\n",
    "1. Microsoft Phi-4 as a smaller LLM via Azure AI Foundry\n",
    "2. Facebook BART-large-CNN as a local transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d8680c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import openai\n",
    "import pydantic_ai\n",
    "import pydantic_ai.direct\n",
    "import pydantic_ai.models.openai\n",
    "import pydantic_ai.providers.azure\n",
    "\n",
    "openai_client = openai.AsyncOpenAI(\n",
    "    base_url=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "phi_4 = pydantic_ai.models.openai.OpenAIChatModel(\n",
    "    model_name=\"Phi-4\",\n",
    "    provider=pydantic_ai.providers.azure.AzureProvider(\n",
    "        openai_client=openai_client\n",
    "    ),\n",
    ")\n",
    "\n",
    "system_prompt = \"\"\"You are a professional summarizer. Create a concise and comprehensive summary of the provided text.\n",
    "\n",
    "Guidelines:\n",
    "- Create a summary that is detailed, thorough, in-depth, and complex, while maintaining clarity and conciseness\n",
    "- Cover all key points and main ideas presented in the original text\n",
    "- Condense the information into an easy-to-understand format\n",
    "- Include relevant details and examples that support the main ideas\n",
    "- Only use information present in the original text\n",
    "- Rely strictly on the provided text, without including external information\n",
    "- Ensure the summary length is appropriate for the complexity of the original text\n",
    "- Organize the summary clearly with well-structured paragraphs\n",
    "- Write in a direct, factual style without conversational language\n",
    "- Do not use meta-references like \"the article\", \"the text\", \"this document\", or \"the author\"\n",
    "- Begin directly with the subject matter itself\"\"\"\n",
    "\n",
    "@dataclass\n",
    "class TokenUsage:\n",
    "    input_tokens: int\n",
    "    completion_tokens: int\n",
    "    total_tokens: int\n",
    "\n",
    "@dataclass\n",
    "class SummaryResult:\n",
    "    text: str\n",
    "    usage: TokenUsage | None\n",
    "\n",
    "async def summarise_with_phi_4(text: str) -> SummaryResult:\n",
    "    response = await pydantic_ai.direct.model_request(\n",
    "        model=phi_4,\n",
    "        messages=[\n",
    "            pydantic_ai.messages.ModelRequest(parts=[\n",
    "                pydantic_ai.messages.SystemPromptPart(system_prompt),\n",
    "                pydantic_ai.messages.UserPromptPart(text)\n",
    "            ])\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    usage = None\n",
    "    if response.usage:\n",
    "        usage = TokenUsage(\n",
    "            input_tokens=response.usage.input_tokens,\n",
    "            completion_tokens=response.usage.output_tokens,\n",
    "            total_tokens=response.usage.total_tokens\n",
    "        )\n",
    "\n",
    "    return SummaryResult(text=response.text, usage=usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6879dff",
   "metadata": {},
   "source": [
    "This uses the HuggingFace `transformers` library to load the BART model and tokeniser.\n",
    "\n",
    "> [!IMPORTANT]\n",
    "> If you cannot access the HuggingFace model hub, you will need to download the model manually and load it from a local path. `notebooks/models/` is a good place to store it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a9ccef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "bart_pipeline = pipeline(\n",
    "    task=\"summarization\",\n",
    "    model=\"facebook/bart-large-cnn\",\n",
    ")\n",
    "\n",
    "def summarise_with_bart(text: str) -> str:\n",
    "    summary = bart_pipeline(\n",
    "        text,\n",
    "        do_sample=False,\n",
    "    )\n",
    "    return summary[0]['summary_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68961e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_article = \"\"\"Nuclear fusion is a process in which two or more atomic nuclei combine to form a single, larger nucleus, accompanied by the release or absorption of energy. This reaction occurs due to the difference in nuclear binding energy between the reactants and the products. The mass difference between the nuclei before and after the reaction is converted into energy, as described by Einstein's equation, (E=mc^2). Fusion is the primary energy source for stars, including the Sun, where hydrogen nuclei fuse to form helium through a series of reactions.\n",
    "\n",
    "For nuclear fusion to occur, extremely high temperatures, pressures, and confinement times are required to overcome the electrostatic repulsion between positively charged nuclei. These conditions are naturally found in stellar cores and are replicated in advanced nuclear weapons and experimental fusion reactors. The \"triple product\" of temperature, density, and confinement time is a critical parameter for achieving sustained fusion.\n",
    "\n",
    "Fusion reactions involving light nuclei, such as deuterium and tritium, are generally exothermic, meaning they release energy. This is because lighter nuclei have a steep positive gradient in the nuclear binding energy curve up to iron and nickel. In contrast, nuclear fission, which involves splitting heavy nuclei, is most energetic for elements like uranium.\n",
    "\n",
    "Applications of nuclear fusion include the development of fusion power as a clean energy source, thermonuclear weapons, neutron sources, and the production of superheavy elements. While fusion offers immense potential as a sustainable energy solution, challenges such as achieving and maintaining the required conditions for ignition and energy gain remain significant.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56055c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_summary = summarise_with_bart(basic_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d174acb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bart_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4aa6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_4_result = await summarise_with_phi_4(basic_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee47ea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(phi_4_result.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265e72c6",
   "metadata": {},
   "source": [
    "We are using the `torchmetrics` library to gain access to numerous evaluation metrics. In this case, we will be using ROUGE and BERTScore to evaluate the quality of the summaries generated by both models.\n",
    "\n",
    "> [!IMPORTANT]\n",
    "> `torchmetrics` also has a dependency on the HuggingFace `transformers` library to load pre-trained models for BERTScore calculation. Like earlier, if you cannot access the HuggingFace model hub, you will need to download the required models manually and load them from a local path. `notebooks/models/` is a good place to store them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423d7579",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.text.rouge import ROUGEScore\n",
    "from torchmetrics.text.bert import BERTScore\n",
    "\n",
    "# Initialize ROUGE scorer\n",
    "rouge_scorer = ROUGEScore()\n",
    "\n",
    "# Initialize BERTScore with local DeBERTa model\n",
    "bert_scorer = BERTScore(model_name_or_path=\"microsoft/deberta-xlarge-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a6745e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "bart_rouge = rouge_scorer(preds=bart_summary, target=basic_article)\n",
    "bart_bert = bert_scorer(preds=bart_summary, target=basic_article)\n",
    "\n",
    "phi_4_rouge = rouge_scorer(preds=phi_4_result.text, target=basic_article)\n",
    "phi_4_bert = bert_scorer(preds=phi_4_result.text, target=basic_article)\n",
    "\n",
    "simple_article_metrics = pd.DataFrame([\n",
    "    {\n",
    "        'article': 'simple_article',\n",
    "        'model': 'BART-large-CNN',\n",
    "        'rougeL_fmeasure': bart_rouge['rougeL_fmeasure'].item(),\n",
    "        'rougeL_precision': bart_rouge['rougeL_precision'].item(),\n",
    "        'rougeL_recall': bart_rouge['rougeL_recall'].item(),\n",
    "        'bert_f1': bart_bert['f1'].item(),\n",
    "        'bert_precision': bart_bert['precision'].item(),\n",
    "        'bert_recall': bart_bert['recall'].item(),\n",
    "    },\n",
    "    {\n",
    "        'article': 'simple_article',\n",
    "        'model': 'Phi-4',\n",
    "        'rougeL_fmeasure': phi_4_rouge['rougeL_fmeasure'].item(),\n",
    "        'rougeL_precision': phi_4_rouge['rougeL_precision'].item(),\n",
    "        'rougeL_recall': phi_4_rouge['rougeL_recall'].item(),\n",
    "        'bert_f1': phi_4_bert['f1'].item(),\n",
    "        'bert_precision': phi_4_bert['precision'].item(),\n",
    "        'bert_recall': phi_4_bert['recall'].item(),\n",
    "    }\n",
    "])[['article', 'model', 'rougeL_fmeasure', 'rougeL_precision', 'rougeL_recall', 'bert_f1', 'bert_precision', 'bert_recall']]\n",
    "\n",
    "simple_article_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153cbe1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style for better-looking plots\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Prepare data for visualization\n",
    "metrics_melted = simple_article_metrics.melt(\n",
    "    id_vars=['article', 'model'],\n",
    "    var_name='metric',\n",
    "    value_name='score'\n",
    ")\n",
    "\n",
    "# Separate ROUGE and BERT metrics\n",
    "rouge_metrics = metrics_melted[metrics_melted['metric'].str.startswith('rougeL')]\n",
    "bert_metrics = metrics_melted[metrics_melted['metric'].str.startswith('bert')]\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# ROUGE-L metrics\n",
    "sns.barplot(data=rouge_metrics, x='metric', y='score', hue='model', ax=axes[0], palette='viridis')\n",
    "axes[0].set_title('ROUGE-L Metrics - Simple Article', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Metric', fontsize=12)\n",
    "axes[0].set_ylabel('Score', fontsize=12)\n",
    "axes[0].set_ylim(0, 1)\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].legend(title='Model')\n",
    "\n",
    "# BERTScore metrics\n",
    "sns.barplot(data=bert_metrics, x='metric', y='score', hue='model', ax=axes[1], palette='magma')\n",
    "axes[1].set_title('BERTScore Metrics - Simple Article', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Metric', fontsize=12)\n",
    "axes[1].set_ylabel('Score', fontsize=12)\n",
    "axes[1].set_ylim(0, 1)\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].legend(title='Model')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f65070d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "\n",
    "import bs4\n",
    "import markdownify\n",
    "\n",
    "ai_oap_url = \"https://www.gov.uk/government/publications/ai-opportunities-action-plan/ai-opportunities-action-plan\"\n",
    "\n",
    "article = requests.get(ai_oap_url)\n",
    "\n",
    "html_content = bs4.BeautifulSoup(article.content, 'html.parser')\n",
    "\n",
    "playbook_content = html_content.find(\"div\", id=\"contents\")\n",
    "\n",
    "article_to_summarise = markdownify.markdownify(str(playbook_content), heading_style=\"ATX\")\n",
    "\n",
    "sections = re.split(r'(?m)^#{1,6} ', article_to_summarise, flags=re.MULTILINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7ebc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chonkie import RecursiveChunker\n",
    "\n",
    "# Initialize the recursive chunker with BART-friendly token limits\n",
    "# BART-large-CNN typically handles up to 1024 tokens\n",
    "chunker = RecursiveChunker(\n",
    "    tokenizer=\"gpt2\",\n",
    "    chunk_size=1024,\n",
    "    min_characters_per_chunk=24,\n",
    ")\n",
    "\n",
    "# Chunk the entire document recursively\n",
    "chunks = chunker.chunk(article_to_summarise)\n",
    "\n",
    "print(f\"Total chunks: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d672881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize each chunk with BART\n",
    "bart_chunk_summaries = []\n",
    "\n",
    "for chunk_idx, chunk in enumerate(chunks):\n",
    "    summary = summarise_with_bart(chunk.text)\n",
    "    bart_chunk_summaries.append({\n",
    "        'chunk_index': chunk_idx,\n",
    "        'summary': summary\n",
    "    })\n",
    "\n",
    "print(f\"Total chunks processed: {len(bart_chunk_summaries)}\")\n",
    "\n",
    "# Map-Reduce: Iteratively reduce summaries until we have a final summary\n",
    "current_summaries = [item['summary'] for item in bart_chunk_summaries]\n",
    "iteration = 1\n",
    "\n",
    "while len(current_summaries) > 1:\n",
    "    print(f\"\\nIteration {iteration}: Reducing {len(current_summaries)} summaries\")\n",
    "    \n",
    "    # Combine all summaries into one text\n",
    "    combined_text = \"\\n\\n\".join(current_summaries)\n",
    "    \n",
    "    # Chunk the combined summaries\n",
    "    reduction_chunks = chunker.chunk(combined_text)\n",
    "    print(f\"Created {len(reduction_chunks)} chunks from combined summaries\")\n",
    "    \n",
    "    # Summarize each chunk\n",
    "    reduced_summaries = []\n",
    "    for chunk in reduction_chunks:\n",
    "        summary = summarise_with_bart(chunk.text)\n",
    "        reduced_summaries.append(summary)\n",
    "    \n",
    "    current_summaries = reduced_summaries\n",
    "    iteration += 1\n",
    "\n",
    "# Final summary\n",
    "bart_final_summary = current_summaries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c6e9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bart_final_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf9dbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the full AI Action Plan document with Phi-4\n",
    "phi_4_ai_oap_result = await summarise_with_phi_4(article_to_summarise)\n",
    "\n",
    "print(phi_4_ai_oap_result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368f79f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(phi_4_ai_oap_result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9923a552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "# Compare summaries using ROUGE-L and BERTScore metrics\n",
    "# 1. Phi-4 output vs original article\n",
    "phi_vs_original_rouge = rouge_scorer(preds=phi_4_ai_oap_result.text, target=article_to_summarise)\n",
    "\n",
    "# Calculate BERTScore by chunking (to avoid memory issues)\n",
    "phi_bert_scores = {'precision': [], 'recall': [], 'f1': []}\n",
    "for chunk in chunks:\n",
    "    chunk_bert = bert_scorer(preds=phi_4_ai_oap_result.text, target=chunk.text)\n",
    "    phi_bert_scores['precision'].append(chunk_bert['precision'].item())\n",
    "    phi_bert_scores['recall'].append(chunk_bert['recall'].item())\n",
    "    phi_bert_scores['f1'].append(chunk_bert['f1'].item())\n",
    "\n",
    "phi_vs_original_bert = {\n",
    "    'precision': statistics.mean(phi_bert_scores['precision']),\n",
    "    'recall': statistics.mean(phi_bert_scores['recall']),\n",
    "    'f1': statistics.mean(phi_bert_scores['f1'])\n",
    "}\n",
    "\n",
    "# 2. BART output vs original article\n",
    "bart_vs_original_rouge = rouge_scorer(preds=bart_final_summary, target=article_to_summarise)\n",
    "\n",
    "# Calculate BERTScore by chunking (to avoid memory issues)\n",
    "bart_bert_scores = {'precision': [], 'recall': [], 'f1': []}\n",
    "for chunk in chunks:\n",
    "    chunk_bert = bert_scorer(preds=bart_final_summary, target=chunk.text)\n",
    "    bart_bert_scores['precision'].append(chunk_bert['precision'].item())\n",
    "    bart_bert_scores['recall'].append(chunk_bert['recall'].item())\n",
    "    bart_bert_scores['f1'].append(chunk_bert['f1'].item())\n",
    "\n",
    "bart_vs_original_bert = {\n",
    "    'precision': statistics.mean(bart_bert_scores['precision']),\n",
    "    'recall': statistics.mean(bart_bert_scores['recall']),\n",
    "    'f1': statistics.mean(bart_bert_scores['f1'])\n",
    "}\n",
    "\n",
    "# 3. BART output vs Phi-4 output\n",
    "bart_vs_phi_rouge = rouge_scorer(preds=bart_final_summary, target=phi_4_ai_oap_result.text)\n",
    "bart_vs_phi_bert = bert_scorer(preds=bart_final_summary, target=phi_4_ai_oap_result.text)\n",
    "\n",
    "ai_oap_metrics = pd.DataFrame([\n",
    "    {\n",
    "        'comparison': 'Phi-4 vs Original',\n",
    "        'rougeL_fmeasure': phi_vs_original_rouge['rougeL_fmeasure'].item(),\n",
    "        'rougeL_precision': phi_vs_original_rouge['rougeL_precision'].item(),\n",
    "        'rougeL_recall': phi_vs_original_rouge['rougeL_recall'].item(),\n",
    "        'bert_f1': phi_vs_original_bert['f1'],\n",
    "        'bert_precision': phi_vs_original_bert['precision'],\n",
    "        'bert_recall': phi_vs_original_bert['recall'],\n",
    "    },\n",
    "    {\n",
    "        'comparison': 'BART vs Original',\n",
    "        'rougeL_fmeasure': bart_vs_original_rouge['rougeL_fmeasure'].item(),\n",
    "        'rougeL_precision': bart_vs_original_rouge['rougeL_precision'].item(),\n",
    "        'rougeL_recall': bart_vs_original_rouge['rougeL_recall'].item(),\n",
    "        'bert_f1': bart_vs_original_bert['f1'],\n",
    "        'bert_precision': bart_vs_original_bert['precision'],\n",
    "        'bert_recall': bart_vs_original_bert['recall'],\n",
    "    },\n",
    "    {\n",
    "        'comparison': 'BART vs Phi-4',\n",
    "        'rougeL_fmeasure': bart_vs_phi_rouge['rougeL_fmeasure'].item(),\n",
    "        'rougeL_precision': bart_vs_phi_rouge['rougeL_precision'].item(),\n",
    "        'rougeL_recall': bart_vs_phi_rouge['rougeL_recall'].item(),\n",
    "        'bert_f1': bart_vs_phi_bert['f1'].item(),\n",
    "        'bert_precision': bart_vs_phi_bert['precision'].item(),\n",
    "        'bert_recall': bart_vs_phi_bert['recall'].item(),\n",
    "    }\n",
    "])[['comparison', 'rougeL_fmeasure', 'rougeL_precision', 'rougeL_recall', 'bert_f1', 'bert_precision', 'bert_recall']]\n",
    "\n",
    "ai_oap_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2b34ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for visualization\n",
    "ai_metrics_melted = ai_oap_metrics.melt(\n",
    "    id_vars=['comparison'],\n",
    "    var_name='metric',\n",
    "    value_name='score'\n",
    ")\n",
    "\n",
    "# Separate ROUGE and BERT metrics\n",
    "ai_rouge_metrics = ai_metrics_melted[ai_metrics_melted['metric'].str.startswith('rougeL')]\n",
    "ai_bert_metrics = ai_metrics_melted[ai_metrics_melted['metric'].str.startswith('bert')]\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# ROUGE-L metrics\n",
    "sns.barplot(data=ai_rouge_metrics, x='comparison', y='score', hue='metric', ax=axes[0], palette='viridis')\n",
    "axes[0].set_title('ROUGE-L Metrics - AI Action Plan Comparisons', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Comparison', fontsize=12)\n",
    "axes[0].set_ylabel('Score', fontsize=12)\n",
    "axes[0].set_ylim(0, 1)\n",
    "axes[0].tick_params(axis='x', rotation=15)\n",
    "axes[0].legend(title='Metric', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# BERTScore metrics\n",
    "sns.barplot(data=ai_bert_metrics, x='comparison', y='score', hue='metric', ax=axes[1], palette='magma')\n",
    "axes[1].set_title('BERTScore Metrics - AI Action Plan Comparisons', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Comparison', fontsize=12)\n",
    "axes[1].set_ylabel('Score', fontsize=12)\n",
    "axes[1].set_ylim(0, 1)\n",
    "axes[1].tick_params(axis='x', rotation=15)\n",
    "axes[1].legend(title='Metric', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
